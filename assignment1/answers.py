lm_stats=[5579336, 0.3116466822608252, 0.9853501906482038, 1.4536908944718608e-06, 0.005158178014182952, 0.348077988641086]
dev_tweets_preds=[False, False, False, False, False, True, True, False, False, False, False, False, True, False, False, False, True, False, False, True, False, True, True, True, True, False]
answer_short_1_3='Padding in N-gram models provide a mean to estimate the likelihood of features appearing at the end or beginning of text without depending on a unigram model.\nLeft padding improves estimates for starting features of text, right padding improves estimates for ending text.\n"Starts" would have higher entropy with padding as "s" is the most common letter at both the start and end of a word.'
answer_short_1_4="p(b|('<s>',)) = [2-gram] 0.046511 Bigram probability of b given <s>\np(b|('b',)) = [2-gram] 0.007750 Bigram probability of b given b \nbacking off for ('b', 'q') Operation performs backoff for b and q\np(q|()) = [1-gram] 0.000892 Unigram probability of q being in the word\np(q|('b',)) = [2-gram] 0.000092 Bigram probability of q given b\np(</s>|('q',)) = [2-gram] 0.010636 Bigram probability of <s> given  q\n7.85102054894183 approximate cross-entropy for the word including padding\nThis set of results shows the probability for each letter in bbq using the trained language model built earlier displaying results for both unigram and bigrams."
answer_short_1_5='These plots show the average entropy across all tweets and plots the number of tweets with the same entropy across 30 bins. \nReferring to the log-scaled graph, around 3-5 is where most tweets lie, and it slopes down to 8-10 where the least tweets have such entropies. \nAfter this, more tweets have increasing entropies until ~18. Higher entropy past the minimum may suggest the tweet may not be English or use non-alphabetic characters.'
top10_ents=[[2.4921691054394848, ['and', 'here', 'is', 'proof', 'the']], [2.5390025889056127, ['and', 'bailed', 'he', 'here', 'is', 'man', 'on', 'that', 'the']], [2.5584079236733106, ['is', 'the', 'this', 'weather', 'worst']], [2.568653427817313, ['s', 's', 's', 's', 's', 's', 's', 's', 's', 's']], [2.569853705187651, ['be', 'bus', 'here', 'the', 'to', 'want']], [2.576919752608039, ['hell', 'that', 'the', 'was', 'wat']], [2.587767243678531, ['creation', 'is', 'of', 'on', 'story', 'the', 'the']], [2.5885860368906832, ['fro', 'one', 'the', 'the', 'with']], [2.595298329492654, ['is', 'money', 'motive', 'the', 'the']], [2.617870705175611, ['at', 'bucks', 'end', 'lead', 'of', 'the', 'the', 'the']]]
bottom10_ents=[[17.523736748003564, ['作品によっては怪人でありながらヒーロー', 'あるいはその逆', 'というシチュエーションも多々ありますが', 'そうした事がやれるのもやはり怪人とヒーローと言うカテゴリが完成しているからだと思うんですよね', 'あれだけのバリエーションがありながららしさを失わないデザインにはまさに感服です']], [17.524868750262904, ['ロンブーの淳さんはスピリチュアルスポット', 'セドナーで瞑想を実践してた', 'これらは偶然ではなく必然的に起こっている', '自然は全て絶好のタイミングで教えてくれている', 'そして今が今年最大の大改革時期だ']], [17.5264931699585, ['実物経済と金融との乖離を際限なく広げる', 'レバレッジが金融で儲けるコツだと', 'まるで正義のように叫ぶ連中が多いけど', 'これほど不健全な金融常識はないと思う', '連中は不健全と知りながら', '他の奴がやるから出し抜かれる前に出し抜くのが道理と言わんばかりに群がる']], [17.527615646393077, ['一応ワンセット揃えてみたんだけど', 'イマイチ効果を感じないのよね', 'それよりはオーラソーマとか', '肉体に直接働きかけるタイプのアプローチの方が効き目を感じ取りやすい', '波動系ならバッチよりはホメオパシーの方がわかりやすい']], [17.53293217459052, ['慶喜ほどの人でさえこうなんだから', '並の人間だったらなおさら参謀無しじゃ何も出来ない', '一般に吹聴されてる慶喜のネガティブ論は', 'こうした敵対勢力による相次ぐテロに対して終始無関心で', '慶喜個人だけに批判を向けがち']], [17.541019489814225, ['昨日のセミナーではお目にかかれて光栄でした', '楽しく充実した時間をありがとうございました', '親しみのもてる分かりやすい講演に勇気を頂きました', '素晴らしいお仕事とともに益々のご活躍願っております', '今後ともよろしくお願いします']], [17.541411086467402, ['自民党が小沢やめろというなら', '当然町村やめろというブーメランがかえってくるわけです', 'おふたりとも選挙で選ばれた正当な国民の代表ですから', 'できればどちらにもやめてほしくありません', 'そろそろこんな不毛なことはやめにしてほしい']], [17.5427257173663, ['知識欲というのは不随意筋でできている', 'どうせ人間には永久に解明できないんだから', '宇宙はある時点で生まれたのか', 'それとも永遠の過去から存在しているのかなんてことを追究するなと言ってもムダだ', '心臓に止まれと命令しても止まらないのと同じことだ']], [17.547644050965395, ['と言いつつもやっぱり笑えない時はあるよなあ', '笑っても自分の笑顔が汚らわしく思えてすぐ止めちゃうの', '自分が息してるだけで悲しくてぼろぼろ泣いてる時期もあった', '今の自分に必要な経験だったとは思うけど', '出来ればあんな感情は二度とごめんだ']], [17.55280652132174, ['中身の羽毛は精製過程で殺菌処理しているから', '羽毛布団からダニが湧くことはない', 'あと羽毛布団の生地は糸の打ち込み本数が多く', '羽毛の吹き出しを防ぐ目つぶし加工をしているからダニは羽毛ふとんの生地を通過できない', 'ただダニが布団に付着することはあるから手入れは必要']]]
answer_essay_question='Problem 1: English is expansive, what words are included in the domain? Do the results want to reflect abbreviations, slang and words that have fallen out of use or into fashion? \nIt is important to define a vocabulary that is representative of what we are interested in. Considering homonyms is also important as for this we would need \ncontextual clues to realise the intended meaning of the word, that is, if we are to consider these words different.\n\nProblem 2: Per word entropy changes depending on context, so to do this across a language depends on the \norder of words and how they are presented. The data we use will greatly impact the result we get. For\nexample, do we use a massive set of professionally written text, or do we include writings that are informal. \nIncluding fictional novels poses another point of dealing with made up words so the data we use is something we must consider.  \n\nProblem 3: How do we ensure that every word that we have stated should be on the vocabulary will be present in the input data for training and testing.\nGiven we must cover every word in the English language, some entropies will be extremes on the scale and will result\nin a skewed result to a larger average entropy so do we want to remove such outliers or are we willing to display results in full.  \n\nExperiment: \n\nData: web-scraped; comprising of written articles, published papers, informal sources (social media, discussion boards etc.), freely available online books and sources of transcribed speech from interviews, speeches and so forth.  \n\nA generative model should be used given the amount of data. A 5-gram would be suitable with Katz Back-off smoothing to ensure we achieve a fair count for all data\nand so that all unseen data is not of equal probabilities. I would split the data into training and testing data at a 70:30 ratio respectively as we want to give the model enough to train on while ensuring \nthe testing set is large enough to cover a full set of possible uses of language.  \n\nEquations:  \n\nKatz P(w_i|w_i-6...w_i-1)  \n\n= d*(C(w_i-6...w_i-1w_i)/(C(w_i-6 ... w_i-1)) if C(w_i-6 ... w_i)>k  \n\n=a*(P(w_i|w_i-7...w_i-1)) otherwise  \n\nWhere a is a weight learned in training  \n\nWith these probabilities I would then use the entropy equation for each word: H(x)=-sum(p(x)longp(x)) \nThen an average of every entropy to produce the final answer. '
naive_bayes_vocab_size=13521
naive_bayes_prior={'V': 0.47766934282005674, 'N': 0.5223306571799433}
naive_bayes_likelihood=[0.006913064743369809, 0.0012190937826217086, 0.12333945519178972, 2.2315401420598457e-06, 2.6766530157362866e-05, 0.004917741586184577, 0.004933935254094318]
naive_bayes_posterior=[{'V': 0.5886037204341108, 'N': 0.41139627956588926}, {'V': 0.1563326709379453, 'N': 0.8436673290620547}, {'V': 0.8124219037837241, 'N': 0.18757809621627583}, {'V': 0.8124219037837241, 'N': 0.18757809621627583}, {'V': 0.9961437486715612, 'N': 0.0038562513284388163}]
naive_bayes_classify=['V', 'N']
naive_bayes_acc=0.7949987620698192
answer_open_question_2_2='1. The differences in accuracy show that for feature extraction using a single feature, [P] is the most useful.\nGiven this, using multiple features proves better than using any sole feature. Just using individual features does not \ntell us about orders or combinations of features so is less accurate. \n2. As accuracies are similar so as Logisitic regression is a discriminative model which preforms worse on larger datasets, \nthis may not be a contributing factor. However, logistic regression \ndoes not assume feature independence like naïve bayes does which makes NB more bias to repeated words. \n3. I would advocate against as it is a very specific case of language that has no guarantee to occur frequently in training and testing data. '
lr_predictions='VVVVVVVVVVVVVVVVVVVVVVVVVVVVVNVVVVVVVVNVNVVVNVNNNNVVNVVNNNVNNNVNNVVVNNVVVNVNVVNVNVNVNNNVVVNVNNNVNVNVNNVNVVVVNNNNVNNNNVVVNVVNVNNVNNVNNVNVNNNVVNVNNVNNNNVNNNNNVNNNNVNNNNVNNNNNVNNNVVVVVVVNVNVNNNNVVVVNVVNVVVNVVNNVVNVNVNNNNVNVVVNVNVNNNNVNNNVNNVNNVNNVNNVNVVNNNNVNVNVNNVNVVNNNNVVNVNVNVVNVNVNVVNVNNVNNVVNNNVNNVVNNNNNVNVNNVVVNNNVVVVNVVVVVNNNNNNVVVNNNNNVNVNVNNVVNVNNNNNVNVNVVNVVVNNNVVVNVVVVVVNNNNNNVVNVVNNNNNNVNNVVVNNVVVVVVVVNNNVNVNVVNNNVVVVVVVVVNVVVVNVNVNNVVNVVVNVNNNNNVVNVNNNVNVNNNVVNVNNVNNNVNVNVNVNVVVVVVNVVNVNNVNNVNNVVVNVNNVVVVVVVVVVVVVVVVVNVVVNVNNVVNVVNNNNVNVNNNVNVNVNVVNNNNNNVVNNNNVVVVVNNVNVNNNNVNVVNVNVVNVNVVNVVVVNVNNNVVNNVVVNNNNVNVVNVVNVNNVVNVVVVNVVVNVNVVVVNNVNVVNVVNNVNNNNVVVNVNVNVVNNVVVVVVNVVVVVVVVVVVVVVVVNVNVVNNVVVVVVVVNNNNNVVNNVNVNNNNVVNVNVVVVNVNVVNVVVNVNNNNNNNVVNVNVNNVVVNNNNNNNVVVNVVVVVVVVNVNVNVVVNVNVVNVNNNVVNVNNNVNVNVNVVNVVNVVNNNNVNNVVNVVVNVVVVNVNVVVVNNNNVNNNVNVNVNNVNVVNNVNNVVVVVNVNNNNVNNNNVNVVVNNNNNVVNVVNNNVNNVNNNNNNVNNNVNVVVNVVVNVVNNVNNNVVNVVVVNVNNNNNVVVNNNVNNVNNNNNNNVVVVNVNNVVVVVNVVVVVVNNVVVNNNVVVNVVNNVNNNNNNNVVVNNNNVVVVVNVNVNVVVNNVNNVNVVNNNVNVNVNNVNNNVVVVVVNNVVNVNNVVVNNNVNVNNNNVNVNNVNNNNNNVVNNVVNNNNNNVVVVVNVNNNVNNNNNVVVVVVVVVNVVVNVNNVNVVNVNNVVNVVNVVVNVVVVVNNNNNNNNNNNNNNNNNVNNNVNNVNNVVVVNVVNVNNNNVVNVVVNNVNNNVVVNNNNVVNNNVNNNVNNNNVVVNVVNVNNNVVVNVNNVVNVVVNVNVNNNNVVNNNNNVVNNVNNNNNNVVVVNNVVVNVNVNNVVVNVNNVVVNVVNVVNVNNVVVNVNVVNNVVVNVVVVVVVVNVVNNNNVNVVNNNVVNNVNNVNVNNVVVNVNNNVVVNNVVVVNNNNVNVNNVVVVVNNNNNNVNVVNVNVVVVVNVNNNNVVNVVVNNVNVVNNVNNNNNVNNNNNNVNNNNNNVNVNNVNVVVNVVNNVNNVVNNNNNNVNNVVNNNVVVNNVNNNVVVNNNNVNNVNVNNNVNNVNNVNNVVNVNNVNNVNNNNVVNVNVVNNVNVNVNVNNVVVVVVNNVNNVNNVVVVNNNVVVVNVNVNVNVNNVNNNNNNVNVNVNNVVNNVNNNVNVVVVNVNNNVNVNVNNVVNVVNVVNNVNNVVVNNNNVVNNVVNNNVNVNNNVVNVNNVNVNVVNNVNVVNVVNVVVNNNVNNNVNNNNNNNNNVNNNVVVNVVVNVVVNVVNVVNVNVVVNNVVNNNNVVNVNVNVNVNVNVVVNVNNNNVVNVNNVNNVNNNNNNNVNVNNNNVNNVVVVNVVVNNNNNVNVNNNVNVNNVVNNVVNVNNNNNVNNVNVVNNNVVNNNVVVNVNVNVVNNVNVNVVVVVNNNVNNNNVVNVNNNNVNNVNNNNNNVNVNVVVVNVVNNNNVNNVNVVNNNNNVVNNVNVNNNNVNNVNNNVNNNNVNNVNVNVVVVNVNVNVNVVVVNVVVVVNVNVNVNVNNNVVVVNVNVNNVVNNNVNNNVNNVNNVNVVNNVNVNNNNNNVNVVVNNNVVVNNNVVVVVNVVVNNVNVNNVVNNVVVNVVNNVNVVVNVVVNVVVVNNVVVNNVVNNVVVVNNNNNVNVVNNNNVNVNVVVNVVNVNVVNNNVVNVNVNVVNNVVVVNNVVVVNNVNVVNNVNNVVVNVNVNVNVNNVVVVVVVVNNVVNVNNNVNVNNNVNNVVVNNNNNNNVVNNVVNVVNVVNVVNVNNVNVVVNNNVNVVVNVNNVVVVNNNNVVVVVNVVNVVVNNNVVVNVVNNVVVNNNVNNNNVVNNNVVVVNNNVNNVVNVNVVNVVNVNVNNVVVVNNVNVNVVVVVNVNNVVNVNNNNNVVNNVNNNVNNNNNVNVVVNVVVVNVVNNNNNVVVNNVVNVNNVVNVVNNVNVNNNNNVVVVNNVNVNNVNNVVVVVNVNVVVVVVVNNNVNNNVNNVVVVNVNVNVNNVVVNNVVNNNNVNNVVNNVNVNNNNVVVNVVNVNVNNVVNVNVVNVNVVVVVNVNVVVNNVVVVNNNVVVVNNVVVNNNVVVNNVVVNNNNNNVVNVNVVVNNVNNVVNVVVNNNNVNVVVVVVVVVNNNNVVNNNNNNVVVNVVVNNNNVVVNVNNVNNNNNVVVNVVNNVNVNNVNNVVVNVNVVVVVVVVNNNNNNNVVNVNVNVVNNVVNVNNVNVNNNNNNVVNNNVVVVVNNNNNVVVNNVNVVVVVVNNVNNVVNNNNVVNNNNVNNNVNNVVVVVNVVVNNVVVVNNNNVNNNVVVNVVVVVVVVNNVNVNVVNNVNVVVVVVNNVVVNVVVNVVNVNVVVNNVNVNVVVVNVNVVNNNVVVNNVVNNNVVNNVNNVNVNVNNVNVNNNNNVNVNVVVVNNNNNNNNNVNVNNVNVNNNNNVNVVNNNNVVVVVNVNNNVVVNVNVVVVVNNNNNNNVVVVVVNVVVVVNVVVVVNNVVNNVVNNVVVVNNNNNVVNVNNVVVNNNNVVNNVNNVVVVNVVNVVVNVNVNVNVNVVNVNVNNNVNVNNNNNVVVVNVVVNNVNNVNNNVNVNNNNNNNVVVVNVNVNVVVVVVVVVVVNVVVVVNNNVNNVNNVVVVNNVNVNNVVVVNNNNVVVVVVNNNNNVNVNNNVNNNVVNVNVVVVVNNVNVNNNNNVVNNNNVVVVVVVVVNVVVVNVVNNVNVNNNNNVNVVVVNVNNNVVVNNVVNNNNNNVVVNVNNVNNNVVNNNVVVVNVVNVVNNNVVNVNNVNVNNVVNNNVNNVNNVVVVNNVNVNNNVNVNVNNVVVVNVNVNNVNNVNNVVVNNNVNNVVVVVVNVNVVVVVVNVNVVVVNNVVVNNVVNNNNVVNNNVNVVVNNVVVNVNNNNVNNNVVVVVVVVVVNVVVNNNNVVVVNVVNNNVNVNNNNNNNVVNVNNNNVVNNVVNNNNVVVNVVNNNVNNVVVVNNNNNVNVVNVVNNNNVNVVNNNNVVVNNNVVNVVNNNNVVVVVVVNVVNNVVNNNNVVVVVVNNVNVVNNVNNVNVVNVVNVVNVVNNNNNVNVVNVNVVVNVNNNVVNVVVNVVVNNVNVNNNNNNVNVVNVVNVVVVVVNNNNNVVNNNNVVVVVVNNNVNNVNNNVVVNNNVVNNVVNVVNVVVNNVNNVVVVVNVVNNVNNVVVVVNVNNVVVNNNNNNVVNNNNVVVNNNNVVNNVNNVNVNVVVVNNNNVNVVNNNNNVNNVVNVVVNNNVVNNNNVVNNNVNVNVNVNVNVVVVVNNNNVVNVNNNVNNNVNNNNNNNVNVVVVNNNVNNNNNNNNVVNVNVVNVVNVNNNVVNNNNNNNVNVNNNVNVNVNVNVNVNNVNVNVNVNVNVVVNNNVNVNNNNNVNNNNNNVNVVVVNNVNNNNNNNNVNNVNNVVNNVVVVVVVNVVNNNVVNVVVNNVVNVNVNVNVVNVVNNVNVVVVVVNNNNNNNNNVNVNNNVNNNNNVNNVNVNNNVVVVNVNVNVNVNVNVVVNNNNNNVVVVNNNNNNNNVVNVVVNVVVNVVVNVVNNVVVVNNVNVNVNVNNVVNVNNVNVVVVNVVNNVNVNNVN'
best10_features=[(-5.452893157762712, "('p', 'of')==1 and label is 'V'"), (4.1036841056034685, "('vpn2', 'seekofinc.')==1 and label is 'V'"), (-3.801520002155064, "('vp', 'roseto')==1 and label is 'N'"), (-3.7472084408124426, "('p', 'without')==1 and label is 'N'"), (3.744834198220542, "('vpn2', 'assumeofdebt')==1 and label is 'V'"), (3.6752292094154617, "('n1p', 'decisionof')==1 and label is 'V'"), (3.427541311919463, "('vpn2', 'followingofthe')==1 and label is 'V'"), (-3.329687580586379, "('n1', 'it')==1 and label is 'N'"), (3.200945298676639, "('vpn2', 'lostto1')==1 and label is 'N'"), (-3.1825170670395453, "('p', 'until')==1 and label is 'N'")]
answer_open_question_2_3='My feature templates include the first four basic features included previously (n1, v, p and n2) but also combinations of p with n1, v and n2. Given the importance of p alone, the combination vp increases accuracy. These combinations highlight correct grammatical terms like "attributed to". Vpn2 is also important to include as it gives "following of the" attachment to the VP instead of two smaller phrases where "following of" could have been NP. \n5.453 (\'p\', \'of\')==1 and label is \'V\'  \nThis feature has the highest absolute weight and therefore the model relies on this the most. This makes sense given the number of occurrences of the word “of” and where it is usually found in text with neighbouring words. \n-3.045 (\'p\', \'via\')==1 and label is \'N\' \nThis feature is interesting as “via” is not a word often related to specific parts of text besides places or methods. Here we can see the N tag which is fair given the words relation to places but, in many cases, “by” can be replaced with “via” in instructions where this would become an incorrect tag.\n4.104 (\'vpn2\', \'seekofinc.\')==1 and label is \'V\'\nThis is the second most important feature to the model and strikes me as an outlier. This is because the term “seek of inc.” is not a phrase you would think appears regularly and not one that you would depend on to make this classification. To me this does not make sense as the models choice for second most important feature in identifying categories.'
